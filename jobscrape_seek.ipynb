{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "def grad_connection_scrape(job_level,discipline,coverletter_context):\n",
    "    \n",
    "    # Set headers to mimic a browser visit\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    base_url = f'https://au.gradconnection.com/{job_level}/{discipline}/australia/'\n",
    "    local_url = 'au.gradconnection.com'\n",
    "    page_num = 1\n",
    "    jobs_list = []\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        if page_num == 1:\n",
    "            url = f\"{base_url}\"\n",
    "        else:\n",
    "            url = f\"{base_url}?page={page_num}\"\n",
    "\n",
    "        # Send a GET request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Break the loop if the request failed\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all job listings\n",
    "        job_listings = soup.find_all('div', class_='box-name')\n",
    "        \n",
    "        # Break the loop if no jobs found\n",
    "        if not job_listings:\n",
    "            print(\"No more job listings found.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        \n",
    "        # Extract details for each job\n",
    "        for job in job_listings:\n",
    "\n",
    "            title = job.find('h3').text\n",
    "            url_extra = job.find('a')['href']\n",
    "\n",
    "            url_new = local_url+url_extra\n",
    "            \n",
    "            company_name = re.search(r'employers/([^/]+)/jobs', url_extra)\n",
    "\n",
    "            if company_name:\n",
    "                company_name = company_name.group(1)\n",
    "                    # Add to jobs list\n",
    "                \n",
    "                # fk pay internship\n",
    "                if company_name!= \"readygrad\" and company_name!= \"gradconnection\":\n",
    "                    # call the api from ollama\n",
    "                    print(\"company_name: \", company_name)\n",
    "                    print(\"title: \", title)\n",
    "                    def remove_think_tags(response):\n",
    "                        # Use regular expression to remove content within <think> tags\n",
    "                        cleaned_response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "                        return cleaned_response.strip()\n",
    "\n",
    "                    api = 'http://localhost:11434/api/generate'\n",
    "                    headers = {'Content-Type': 'application/json'}\n",
    "                    prompt = f'My name is Richard Shao, I am a {coverletter_context[\"year_level\"]} student majoring in {coverletter_context[\"major\"]} with a minor in {coverletter_context[\"minor\"]}.I have experience in {coverletter_context[\"ability\"]}. One of the relevant event to the company program is {coverletter_context[\"event\"]}. The company I am applying for is {company_name} and the program is {title}. Please write a cover letter for the company {company_name} and the program {title}. Make sure that the cover letter is relevant to the context, and fits the company culture. Please keep the cover letter to one page with is around 500 words. There is no need to mention anything else other than your name on the top of a cover letter. Please double check that the company name is correct. Start the response with: Dear Hiring Manager,and end with Richard Shao'\n",
    "\n",
    "                    data = {\n",
    "                        'model': 'deepseek-r1:14b',\n",
    "                        'prompt': prompt,\n",
    "                        'stream': False\n",
    "                    }\n",
    "\n",
    "                    response = requests.post(api, headers=headers, data=json.dumps(data))\n",
    "                    result = remove_think_tags(response.json()[\"response\"])\n",
    "\n",
    "                    import os\n",
    "\n",
    "                    # Define your target directory and file name\n",
    "                    directory = \"coverletters_grad\"\n",
    "                    filename = f\"{company_name}_{title}.txt\"\n",
    "                    # remove illegal character in filename\n",
    "                    allowed_fn_chars = [' ','_']\n",
    "                    filename = \"\".join(x for x in filename if (x.isalnum() or x in allowed_fn_chars))\n",
    "\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "\n",
    "                    # Create the directory if it doesn't exist\n",
    "                    os.makedirs(directory, exist_ok=True)\n",
    "                    \n",
    "                    # export result into a txt file named by company name and job title\n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(result)\n",
    "                    \n",
    "                    jobs_list.append({\n",
    "                        'Program Title': title,\n",
    "                        'Company': company_name,\n",
    "                        'Link': url_new\n",
    "                    })\n",
    "        \n",
    "        # Increment page number and add a delay\n",
    "        page_num += 1\n",
    "        time.sleep(1)  # 1-second delay between requests\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_jobs = pd.DataFrame(jobs_list)\n",
    "    df_jobs.to_excel(f'{discipline}_{job_level}_jobs.xlsx', index=False)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nScraping completed. Found {} relevant jobs.\".format(len(df_jobs)))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "company_name:  the-alternative\n",
      "title:  The Alternative\n",
      "company_name:  bcg\n",
      "title:  Boston Consulting Group | Graduate Associate – Consulting careers (Maths & Data)\n",
      "company_name:  anao\n",
      "title:  Data Analyst and IT Auditor Graduate Stream\n",
      "company_name:  fdm-group\n",
      "title:  Technical Analyst Graduate Program\n",
      "company_name:  fdm-group\n",
      "title:  Expression of interest Gold Coast: Graduate Software Engineering Program – Major Australian bank\n",
      "company_name:  fdm-group\n",
      "title:  Expression of Interest: Project Grace Graduate Program (Gold Coast)\n",
      "company_name:  fdm-group\n",
      "title:  Data & Analytics Graduate Program\n",
      "company_name:  fdm-group\n",
      "title:  Software Engineering Graduate Program\n",
      "company_name:  optiver\n",
      "title:  FutureFocus – Technology 2025\n",
      "company_name:  optiver\n",
      "title:  FutureFocus – Trading & Research 2025\n",
      "company_name:  fdm-group\n",
      "title:  Tech Unboxed Series: Skills, Careers & AI Hacks\n",
      "company_name:  amazon\n",
      "title:  Amazon WoW presents, MARCH FORWARD : Accelerating Equality Together! #IWD 2025\n",
      "company_name:  anz\n",
      "title:  2026 ANZ Australia Graduate Program - Expression of Interest\n",
      "company_name:  the-alternative\n",
      "title:  The Alternative\n",
      "company_name:  ey\n",
      "title:  Join the EY Talent Community!\n",
      "company_name:  nab\n",
      "title:  NAB 2026 Graduate Program - Expression of Interest\n",
      "company_name:  amazon\n",
      "title:  Amazon Expression of Interest\n",
      "company_name:  nsw-government\n",
      "title:  NSW Government Graduate Program - EOI\n",
      "company_name:  amazon\n",
      "title:  2025 Grad Operations Shift Manager (NSW)\n",
      "company_name:  csiro\n",
      "title:  2026 CSIRO Graduate Program - Expression of Interest\n",
      "Scraping page 2...\n",
      "company_name:  nestle\n",
      "title:  Nestle Youth Entrepreneurship Platform (YEP)\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "No more job listings found.\n",
      "\n",
      "Scraping completed. Found 21 relevant jobs.\n"
     ]
    }
   ],
   "source": [
    "# please choose from the following job level: graduate-jobs, internships，entry-level-jobs\n",
    "discipline = \"data-science-and-analytics\"\n",
    "\n",
    "# choose your discipline \"data-science-and-analytics\", \"computer-science\"\n",
    "job_level = 'graduate-jobs'\n",
    "\n",
    "# cover letter context, year_level, major, minor, interest,company_name, job_title company_description\n",
    "ability = \"different programing languages including R,python,java,C,SQL. Differnet packages include numpy, pandas, matplotlib, ggplot2,pytorch,scikit-learn,spark\"\n",
    "event = \"help processing data from various news sources, and help to build a model to extract emotion and company name from the news as well as putting a label to the news \"\n",
    "coverletter_context = {\"year_level\":\"first year master\",\"major\":\"Probability and Statisitcs\",\n",
    "                       \"minor\":\"Data Science\",\"company_name\":\"\",\n",
    "                       \"job_program\":\"\",\"ability\":ability,\"event\":event}\n",
    "\n",
    "grad_connection_scrape(job_level,discipline,coverletter_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.seek.com.au/internship-machine-learning-jobs/in-All-Australia?classification=1223%2C6281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Analysing page 1...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# please choose from the following job level: graduate-jobs, internships，entry-level-jobs\n",
    "seek_keyword = f'{job_level}-machine-learning'\n",
    "\n",
    "# Set headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize list to store job data and keywords for filtering\n",
    "jobs_list = []\n",
    "\n",
    "\n",
    "# choose your discipline (see seek website for details)\n",
    "discipline = \"1223%2C6281\" # All ICT and Science&Technology\n",
    "word_discipline = \"science_and_technology\"\n",
    "base_url = f'https://www.seek.com.au/{seek_keyword}-jobs/in-All-Australia?classification={discipline}'\n",
    "local_url = 'https://www.seek.com.au'\n",
    "page_num = 1\n",
    "\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    if page_num == 1:\n",
    "        url = f\"{base_url}\"\n",
    "    else:\n",
    "        url = f\"{base_url}&page={page_num}\"\n",
    "\n",
    "    # Send a GET request\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Break the loop if the request failed\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "        break\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all job listings\n",
    "    job_listings = soup.find_all('article')\n",
    "    # Break the loop if no jobs found\n",
    "    if not job_listings:\n",
    "        print(\"No more job listings found.\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Analysing page {page_num}...\")\n",
    "    \n",
    "    # Extract details for each job\n",
    "    for job in job_listings:\n",
    "\n",
    "        title = job.get('aria-label')\n",
    "        url_extra = job.find('a')['href']\n",
    "\n",
    "        url_new = local_url+url_extra\n",
    "        \n",
    "        company = job.find('a', {'data-type': 'company'})\n",
    "        if company:\n",
    "            company_name = company.text\n",
    "        else:\n",
    "            company_name = None\n",
    "\n",
    "        if company_name:\n",
    "            # company_name = company_name.group(1)\n",
    "                # Add to jobs list\n",
    "               \n",
    "            # fk pay internship\n",
    "            if company_name!= \"readygrad\" and company_name!= \"gradconnection\":\n",
    "                def remove_think_tags(response):\n",
    "                        # Use regular expression to remove content within <think> tags\n",
    "                        cleaned_response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "                        return cleaned_response.strip()\n",
    "\n",
    "                api = 'http://localhost:11434/api/generate'\n",
    "                headers = {'Content-Type': 'application/json'}\n",
    "                prompt = f'My name is Richard Shao, I am a {coverletter_context[\"year_level\"]} student majoring in {coverletter_context[\"major\"]} with a minor in {coverletter_context[\"minor\"]}.I have experience in {coverletter_context[\"ability\"]}. One of the relevant event to the company program is {coverletter_context[\"event\"]}. The company I am applying for is {company_name} and the program is {title}. Please write a cover letter for the company {company_name} and the program {title}. Make sure that the cover letter is relevant to the context, and fits the company culture. Please keep the cover letter to one page with is around 500 words. There is no need to mention anything else other than your name on the top of a cover letter. Please double check that the company name is correct. Start the response with: Dear Hiring Manager,and end with Richard Shao'\n",
    "\n",
    "                data = {\n",
    "                    'model': 'deepseek-r1:14b',\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False\n",
    "                }\n",
    "\n",
    "                response = requests.post(api, headers=headers, data=json.dumps(data))\n",
    "                result = remove_think_tags(response.json()[\"response\"])\n",
    "\n",
    "                import os\n",
    "\n",
    "                # Define your target directory and file name\n",
    "                directory = \"coverletters_seek\"\n",
    "                filename = f\"{company_name}_{title}.txt\"\n",
    "                # remove illegal character in filename\n",
    "                allowed_fn_chars = [' ','_']\n",
    "                filename = \"\".join(x for x in filename if (x.isalnum() or x in allowed_fn_chars))\n",
    "\n",
    "                file_path = os.path.join(directory, filename)\n",
    "\n",
    "                # Create the directory if it doesn't exist\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "                \n",
    "                # export result into a txt file named by company name and job title\n",
    "                with open(file_path, 'w') as f:\n",
    "                    f.write(result)\n",
    "                jobs_list.append({\n",
    "                    'Program Title': title,\n",
    "                    'Company': company_name,\n",
    "                    'Link': url_new,\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Increment page number and add a delay\n",
    "    page_num += 1\n",
    "    time.sleep(5)  # 1-second delay between requests\n",
    "\n",
    "\n",
    "# append into df_jobs\n",
    "df_jobs = pd.DataFrame(jobs_list)\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"\\nScraping completed. Found {} relevant jobs.\".format(len(df_jobs)))\n",
    "print(\"\\nSample of scraped jobs:\")\n",
    "\n",
    "\n",
    "\n",
    "#To save to CSV:\n",
    "df_jobs.to_excel(f'{word_discipline}_{job_level}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datascience_graduate-jobs to excel\n",
    "df_jobs.to_excel(f'datascience_{job_level}.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
