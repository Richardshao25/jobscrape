{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.parser import parse\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "class CustomError(Exception):\n",
    "    pass\n",
    "\n",
    "def grad_connection_scrape(job_level,discipline,coverletter_context):\n",
    "    \n",
    "    # Set headers to mimic a browser visit\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    base_url = f'https://au.gradconnection.com/{job_level}/{discipline}/australia/'\n",
    "    local_url = 'https://au.gradconnection.com'\n",
    "    page_num = 1\n",
    "    jobs_list = []\n",
    "\n",
    "    while True:\n",
    "        # Construct the URL for the current page\n",
    "        if page_num == 1:\n",
    "            url = f\"{base_url}\"\n",
    "        else:\n",
    "            url = f\"{base_url}?page={page_num}\"\n",
    "\n",
    "        # Send a GET request\n",
    "        response = requests.get(url, headers=headers,verify=False)\n",
    "        \n",
    "        # Break the loop if the request failed\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all job listings\n",
    "        job_listings = soup.find_all('a', class_='box-header-title')\n",
    "\n",
    "        # Break the loop if no jobs found\n",
    "        if not job_listings:\n",
    "            print(\"No more job listings found.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        \n",
    "        # Extract details for each job\n",
    "        index = 0\n",
    "        for job in job_listings:\n",
    "            \n",
    "            job_type = None\n",
    "            disciplines = None\n",
    "            work_rights = None\n",
    "            work_from_home = None\n",
    "            location = None\n",
    "            international = None\n",
    "            closing_date = None\n",
    "            position_start_date = None\n",
    "            \n",
    "            # finds all the secondary link to the job\n",
    "            job_link = str(job.get('href'))\n",
    "            if not \"notifyme\" in job_link:\n",
    "                current_url = local_url+job_link\n",
    "                try:\n",
    "                    response2 = requests.get(current_url, headers=headers,verify = False)\n",
    "                except CustomError as e:\n",
    "                    print(f\"Custom error caught: {e}\")\n",
    "\n",
    "                # soup2 has all the info about the job\n",
    "                soup2 = BeautifulSoup(response2.content, 'html.parser')\n",
    "\n",
    "                # job details contains all info about the job\n",
    "                job_detail = soup2.find_all('li',class_ = 'box-content-catagories catagories-list')\n",
    "                for detail in job_detail:\n",
    "                    #print(detail)\n",
    "                    strong_tag = detail.find('strong',class_ = 'box-content-catagories-bold').get_text()\n",
    "                    #print(\"strong_tag\",strong_tag)\n",
    "\n",
    "                \n",
    "                    if strong_tag:\n",
    "                        if strong_tag == \"Job type:\":\n",
    "                            job_type = detail.text[9:]\n",
    "                            #print(job_type)\n",
    "                        elif strong_tag == \"Disciplines:\":\n",
    "                            disciplines = detail.text[11:]\n",
    "                            #print(disciplines)\n",
    "                        elif strong_tag == \"Work from home:\":\n",
    "                            work_from_home = detail.text[15:]\n",
    "                            #print(work_from_home)\n",
    "                        elif strong_tag == \"Locations:\":\n",
    "                            location = detail.text[10:]\n",
    "                            #print(location)\n",
    "                        elif strong_tag == \"ACCEPTS INTERNATIONAL\":\n",
    "                            international = detail.text[21:]\n",
    "                            #print(international)\n",
    "                        elif strong_tag == \"Closing Date:\":\n",
    "                            closing_date = detail.text[13:]\n",
    "                            closing_date = parse(closing_date)\n",
    "                            #print(closing_date)\n",
    "                        elif strong_tag == \"Position Start Date:\":\n",
    "                            position_start_date = detail.text[20:]\n",
    "                            \n",
    "                            #print(position_start_date)\n",
    "                \n",
    "                    company_name = soup2.find('h1',class_ = 'employers-panel-title').get_text()\n",
    "                    program_name = soup2.find('h1',class_ = 'employers-profile-h1').get_text()\n",
    "                    \n",
    "                # fk pay internship\n",
    "                if company_name!= \"Readygrad\" and company_name!= \"GradConnection\" and company_name!=\"CareerDC\" and company_name!=\"Premium Graduate Placements\":\n",
    "                    # call the api from ollama\n",
    "                    #print(\"company_name: \", company_name)\n",
    "                    #print(\"title: \", title)\n",
    "                    # def remove_think_tags(response):\n",
    "                    #     # Use regular expression to remove content within <think> tags\n",
    "                    #     cleaned_response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "                    #     return cleaned_response.strip()\n",
    "\n",
    "                    # api = 'http://localhost:11434/api/generate'\n",
    "                    # headers = {'Content-Type': 'application/json'}\n",
    "                    # prompt = f'My name is Richard Shao, I am a {coverletter_context[\"year_level\"]} student majoring in {coverletter_context[\"major\"]} with a minor in {coverletter_context[\"minor\"]}.I have experience in {coverletter_context[\"ability\"]}. One of the relevant event to the company program is {coverletter_context[\"event\"]}. The company I am applying for is {company_name} and the program is {title}. Please write a cover letter for the company {company_name} and the program {title}. Make sure that the cover letter is relevant to the context, and fits the company culture. Please keep the cover letter to one page with is around 500 words. There is no need to mention anything else other than your name on the top of a cover letter. Please double check that the company name is correct. Start the response with: Dear Hiring Manager,and end with Richard Shao'\n",
    "\n",
    "                    # data = {\n",
    "                    #     'model': 'deepseek-r1:14b',\n",
    "                    #     'prompt': prompt,\n",
    "                    #     'stream': False\n",
    "                    # }\n",
    "\n",
    "                    # response = requests.post(api, headers=headers, data=json.dumps(data))\n",
    "                    # result = remove_think_tags(response.json()[\"response\"])\n",
    "\n",
    "                    # import os\n",
    "\n",
    "                    # # Define your target directory and file name\n",
    "                    # directory = \"coverletters_grad\"\n",
    "                    # filename = f\"{company_name}_{title}.txt\"\n",
    "                    # # remove illegal character in filename\n",
    "                    # allowed_fn_chars = [' ','_']\n",
    "                    # filename = \"\".join(x for x in filename if (x.isalnum() or x in allowed_fn_chars))\n",
    "\n",
    "                    # file_path = os.path.join(directory, filename)\n",
    "\n",
    "                    # # Create the directory if it doesn't exist\n",
    "                    # os.makedirs(directory, exist_ok=True)\n",
    "                    \n",
    "                    # # export result into a txt file named by company name and job title\n",
    "                    # with open(file_path, 'w') as f:\n",
    "                    #     f.write(result)\n",
    "                    \n",
    "                    jobs_list.append({\n",
    "                        'Program Title': program_name,\n",
    "                        'Company': company_name,\n",
    "                        'Link': current_url,\n",
    "                        'Job Type': job_type,\n",
    "                        'Disciplines': disciplines,\n",
    "                        'Work from Home': work_from_home,\n",
    "                        'Location': location,\n",
    "                        'International': international,\n",
    "                        'Position Start Date': position_start_date,\n",
    "                        'Closing Date': closing_date\n",
    "                    })\n",
    "                    time.sleep(1)\n",
    "                    print(program_name,company_name)\n",
    "                    print(f'onto the {index}th job')\n",
    "                    index+=1\n",
    "                    # if index == 6:\n",
    "                    #     time.sleep(60)\n",
    "    \n",
    "        # Increment page number and add a delay\n",
    "        page_num += 1\n",
    "        time.sleep(2)  # 1-second delay between requests\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_jobs = pd.DataFrame(jobs_list)\n",
    "    df_jobs.sort_values(by='Closing Date', inplace=True)\n",
    "    df_jobs.to_excel(f'{discipline}_{job_level}_jobs.xlsx', index=False)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nScraping completed. Found {} relevant jobs.\".format(len(df_jobs)))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "EY 2025/26 Vacationer Data Analytics Program EY\n",
      "onto the 0th job\n",
      "Quantitative Researcher Internship - 2025-26 Optiver\n",
      "onto the 1th job\n",
      "Quantitative Trading Internship - 2025/26 Optiver\n",
      "onto the 2th job\n",
      "EY 2025/26 Vacationer Information Security Program EY\n",
      "onto the 3th job\n",
      "2025 Winter Development Program (Perth) Macquarie Group\n",
      "onto the 4th job\n",
      "2026 KPMG Vacationer Program – Risk KPMG\n",
      "onto the 5th job\n",
      "2026 KPMG Vacationer Program – Technology and Digital KPMG\n",
      "onto the 6th job\n",
      "2026 KPMG Vacationer Program – Engineering KPMG\n",
      "onto the 7th job\n",
      "EY 2025/26 Vacationer Information Technology / Information Systems / Business Information Systems Program EY\n",
      "onto the 8th job\n",
      "2025 Winter Development Program (Melbourne) Macquarie Group\n",
      "onto the 9th job\n",
      "2025 Winter Development Program (Sydney) Macquarie Group\n",
      "onto the 10th job\n",
      "2026 KPMG Vacationer Program – Business and Consulting KPMG\n",
      "onto the 11th job\n",
      "2025-2026 NAB Summer Intern Program - Expression of Interest NAB\n",
      "onto the 12th job\n",
      "NSW Government Virtual Internship Program NSW Government\n",
      "onto the 13th job\n",
      "Scraping page 2...\n",
      "Deloitte STEM Connect Virtual Experience Deloitte\n",
      "onto the 0th job\n",
      "CommBank: Virtual Experience Program – Forage CommBank\n",
      "onto the 1th job\n",
      "Scraping page 3...\n",
      "No more job listings found.\n",
      "\n",
      "Scraping completed. Found 16 relevant jobs.\n"
     ]
    }
   ],
   "source": [
    "# please choose from the following job level: graduate-jobs, internships，entry-level-jobs\n",
    "discipline = \"data-science-and-analytics\"\n",
    "\n",
    "# choose your discipline \"data-science-and-analytics\", \"computer-science\"\n",
    "job_level = 'internships'\n",
    "\n",
    "# cover letter context, year_level, major, minor, interest,company_name, job_title company_description\n",
    "ability = \"different programing languages including R,python,java,C,SQL. Differnet packages include numpy, pandas, matplotlib, ggplot2,pytorch,scikit-learn,spark\"\n",
    "event = \"help processing data from various news sources, and help to build a model to extract emotion and company name from the news as well as putting a label to the news \"\n",
    "coverletter_context = {\"year_level\":\"first year master\",\"major\":\"Probability and Statisitcs\",\n",
    "                       \"minor\":\"Data Science\",\"company_name\":\"\",\n",
    "                       \"job_program\":\"\",\"ability\":ability,\"event\":event}\n",
    "\n",
    "grad_connection_scrape(job_level,discipline,coverletter_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.seek.com.au/internship-machine-learning-jobs/in-All-Australia?classification=1223%2C6281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Analysing page 1...\n",
      "Scraping page 2...\n",
      "Analysing page 2...\n",
      "Scraping page 3...\n",
      "Analysing page 3...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 89\u001b[0m\n\u001b[1;32m     81\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMy name is Richard Shao, I am a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoverletter_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear_level\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m student majoring in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoverletter_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with a minor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoverletter_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.I have experience in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoverletter_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mability\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. One of the relevant event to the company program is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoverletter_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The company I am applying for is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the program is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please write a cover letter for the company \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Make sure that the cover letter is relevant to the context, and fits the company culture. Please keep the cover letter to one page with is around 500 words. There is no need to mention anything else other than your name on the top of a cover letter. Please double check that the company name is correct. Start the response with: Dear Hiring Manager,and end with Richard Shao\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     83\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeepseek-r1:14b\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     87\u001b[0m }\n\u001b[0;32m---> 89\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m result \u001b[38;5;241m=\u001b[39m remove_think_tags(response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# please choose from the following job level: graduate-jobs, internships，entry-level-jobs\n",
    "seek_keyword = f'{job_level}-machine-learning'\n",
    "\n",
    "# Set headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Initialize list to store job data and keywords for filtering\n",
    "jobs_list = []\n",
    "\n",
    "\n",
    "# choose your discipline (see seek website for details)\n",
    "discipline = \"1223%2C6281\" # All ICT and Science&Technology\n",
    "word_discipline = \"science_and_technology\"\n",
    "base_url = f'https://www.seek.com.au/{seek_keyword}-jobs/in-All-Australia?classification={discipline}'\n",
    "local_url = 'https://www.seek.com.au'\n",
    "page_num = 1\n",
    "\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    if page_num == 1:\n",
    "        url = f\"{base_url}\"\n",
    "    else:\n",
    "        url = f\"{base_url}&page={page_num}\"\n",
    "\n",
    "    # Send a GET request\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Break the loop if the request failed\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "        break\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all job listings\n",
    "    job_listings = soup.find_all('article')\n",
    "    # Break the loop if no jobs found\n",
    "    if not job_listings:\n",
    "        print(\"No more job listings found.\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Analysing page {page_num}...\")\n",
    "    \n",
    "    # Extract details for each job\n",
    "    for job in job_listings:\n",
    "\n",
    "        title = job.get('aria-label')\n",
    "        url_extra = job.find('a')['href']\n",
    "\n",
    "        url_new = local_url+url_extra\n",
    "        \n",
    "        company = job.find('a', {'data-type': 'company'})\n",
    "        if company:\n",
    "            company_name = company.text\n",
    "        else:\n",
    "            company_name = None\n",
    "\n",
    "        if company_name:\n",
    "            # company_name = company_name.group(1)\n",
    "                # Add to jobs list\n",
    "               \n",
    "            # fk pay internship\n",
    "            if company_name!= \"readygrad\" and company_name!= \"gradconnection\":\n",
    "                def remove_think_tags(response):\n",
    "                        # Use regular expression to remove content within <think> tags\n",
    "                        cleaned_response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "                        return cleaned_response.strip()\n",
    "\n",
    "                api = 'http://localhost:11434/api/generate'\n",
    "                headers = {'Content-Type': 'application/json'}\n",
    "                prompt = f'My name is Richard Shao, I am a {coverletter_context[\"year_level\"]} student majoring in {coverletter_context[\"major\"]} with a minor in {coverletter_context[\"minor\"]}.I have experience in {coverletter_context[\"ability\"]}. One of the relevant event to the company program is {coverletter_context[\"event\"]}. The company I am applying for is {company_name} and the program is {title}. Please write a cover letter for the company {company_name} and the program {title}. Make sure that the cover letter is relevant to the context, and fits the company culture. Please keep the cover letter to one page with is around 500 words. There is no need to mention anything else other than your name on the top of a cover letter. Please double check that the company name is correct. Start the response with: Dear Hiring Manager,and end with Richard Shao'\n",
    "\n",
    "                data = {\n",
    "                    'model': 'deepseek-r1:14b',\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False\n",
    "                }\n",
    "\n",
    "                response = requests.post(api, headers=headers, data=json.dumps(data))\n",
    "                result = remove_think_tags(response.json()[\"response\"])\n",
    "\n",
    "                import os\n",
    "\n",
    "                # Define your target directory and file name\n",
    "                directory = \"coverletters_seek\"\n",
    "                filename = f\"{company_name}_{title}.txt\"\n",
    "                # remove illegal character in filename\n",
    "                allowed_fn_chars = [' ','_']\n",
    "                filename = \"\".join(x for x in filename if (x.isalnum() or x in allowed_fn_chars))\n",
    "\n",
    "                file_path = os.path.join(directory, filename)\n",
    "\n",
    "                # Create the directory if it doesn't exist\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "                \n",
    "                # export result into a txt file named by company name and job title\n",
    "                with open(file_path, 'w') as f:\n",
    "                    f.write(result)\n",
    "                jobs_list.append({\n",
    "                    'Program Title': title,\n",
    "                    'Company': company_name,\n",
    "                    'Link': url_new,\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Increment page number and add a delay\n",
    "    page_num += 1\n",
    "    time.sleep(5)  # 1-second delay between requests\n",
    "\n",
    "\n",
    "# append into df_jobs\n",
    "df_jobs = pd.DataFrame(jobs_list)\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"\\nScraping completed. Found {} relevant jobs.\".format(len(df_jobs)))\n",
    "print(\"\\nSample of scraped jobs:\")\n",
    "\n",
    "\n",
    "\n",
    "#To save to CSV:\n",
    "df_jobs.to_excel(f'{word_discipline}_{job_level}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datascience_graduate-jobs to excel\n",
    "df_jobs.to_excel(f'datascience_{job_level}.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
